{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refinement and G-Prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import copy\n",
    "import plotly.graph_objects as go\n",
    "import math\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.nn.utils import parameters_to_vector\n",
    "\n",
    "from data.uci_datasets import UCIData\n",
    "from main import set_seed, get_device\n",
    "from models.nets import create_mlp\n",
    "from trainer import ModelTrainer, NegativeLogLikelihood\n",
    "\n",
    "from laplace import Laplace\n",
    "\n",
    "from backpack import backpack, extend\n",
    "from backpack.extensions import BatchGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "from omegaconf import OmegaConf\n",
    "try:\n",
    "    initialize(version_base=None, config_path=\"configuration\")\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "config = compose(config_name=\"uci.yaml\")\n",
    "set_seed(config.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refined Laplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_nn(model):\n",
    "    weights = []\n",
    "    for name, param in model.named_parameters():\n",
    "        weights.append(param.detach().flatten())\n",
    "    return torch.cat(weights, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RefinedLaplace Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RefinedLaplace(nn.Module):\n",
    "    def __init__(self, model, output_dim, posterior_covariance):\n",
    "        super(RefinedLaplace, self).__init__()\n",
    "        self.model = model\n",
    "        self.output_dim = output_dim\n",
    "        self.weights_map = flatten_nn(self.model)\n",
    "        self.weights = torch.nn.Parameter(torch.clone(self.weights_map), requires_grad=True)\n",
    "        self.posterior_covariance = posterior_covariance\n",
    "\n",
    "    def get_parameters(self):\n",
    "        return self.weights\n",
    "\n",
    "    def forward(self, X):\n",
    "        with torch.no_grad():\n",
    "            f = self.model(X)\n",
    "        J = self._jacobian(X) \n",
    "        out = torch.einsum(\"ijk,k->ij\", J, (self.weights - self.weights_map)) + f\n",
    "        return out\n",
    "    \n",
    "    def predict(self, X):\n",
    "        with torch.no_grad():\n",
    "            f = self.model(X)\n",
    "        J = self._jacobian(X)\n",
    "        mean = torch.einsum(\"ijk,k->ij\", J, (self.weights - self.weights_map)) + f\n",
    "        return mean, self._functional_variance(J)\n",
    "        \n",
    "    def _functional_variance(self, Js):\n",
    "        return torch.einsum('ncp,pq,nkq->nck', Js, self.posterior_covariance, Js)\n",
    " \n",
    "\n",
    "    def _jacobian(self, X):\n",
    "        \"\"\"\n",
    "        Compute the jacobian of the model with respect to the input X\n",
    "        Args:\n",
    "            X: input tensor\n",
    "        Returns:\n",
    "            J: jacobian of the model with respect to X\n",
    "        \"\"\"\n",
    "        model = copy.deepcopy(self.model)\n",
    "        model.eval()\n",
    "        model = extend(model)\n",
    "        Js = []\n",
    "        for o in range(self.output_dim):\n",
    "            f = model(X)\n",
    "            f_o = f.sum(dim=0)[o]\n",
    "\n",
    "            with backpack(BatchGrad()):\n",
    "                f_o.backward()\n",
    "            Jo = []\n",
    "            for name, param in model.named_parameters():    \n",
    "                batch_size = param.grad_batch.size(0)\n",
    "                Jo.append(param.grad_batch.reshape(batch_size, -1))\n",
    "            Jo = torch.cat(Jo, dim=1)\n",
    "            Js.append(Jo)\n",
    "        return torch.stack(Js, dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainer for refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(y, mu, std):\n",
    "    dist = Normal(mu.squeeze(), std.squeeze())\n",
    "    log_probs = dist.log_prob(y.squeeze())\n",
    "    return log_probs.squeeze().sum().item()\n",
    "\n",
    "def evaluate_predictive(model, sigma, dataloader, device):\n",
    "    ll = 0.0\n",
    "    count = 0\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        f_mu, f_var = model.predict(X)\n",
    "        f_sigma = torch.sqrt(f_var)\n",
    "        pred_std = torch.sqrt(f_sigma**2 + sigma**2)\n",
    "        ll += log_likelihood(y, f_mu, pred_std)\n",
    "        count += X.shape[0]\n",
    "    return -ll / count\n",
    "\n",
    "def evaluate(model, sigma, dataloader, device):\n",
    "    model.eval()\n",
    "    criteria = NegativeLogLikelihood(sigma=sigma).to(device)\n",
    "    err = 0.0\n",
    "    nll = 0.0\n",
    "    count = 0\n",
    "    for X, y in dataloader:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        batch_size = X.shape[0]\n",
    "        out = model(X)\n",
    "        loss = criteria(out, y).mean()\n",
    "        err += F.mse_loss(out, y, reduction=\"mean\").sqrt().item() * batch_size\n",
    "        nll += loss.item() * batch_size\n",
    "        count += batch_size\n",
    "\n",
    "    nll = nll / count\n",
    "    err = err / count\n",
    "\n",
    "    return nll, err\n",
    "\n",
    "def train(model, sigma, delta, train_dataloader, val_dataloader, epochs, lr, device):\n",
    "    criteria = NegativeLogLikelihood(sigma=sigma).to(device)\n",
    "    delta = delta\n",
    "    theta = model.get_parameters()\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [model.get_parameters()], lr=lr\n",
    "    )\n",
    "\n",
    "    best_val_nll = math.inf\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_epoch = 0\n",
    "    for i in range(epochs):\n",
    "        epoch_err = 0.0\n",
    "        epoch_nll = 0.0\n",
    "        count = 0\n",
    "        model.train()\n",
    "        for X, y in train_dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(X)\n",
    "            loss = criteria(out, y).sum() + (0.5 * (delta * theta) @ theta) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            batch_size = X.shape[0]\n",
    "            epoch_err += (\n",
    "                F.mse_loss(out, y, reduction=\"mean\").sqrt().item() * batch_size\n",
    "            )\n",
    "            epoch_nll += loss \n",
    "            count += batch_size\n",
    "\n",
    "        epoch_nll = epoch_nll / count\n",
    "        epoch_err = epoch_err / count\n",
    "        val_nll, val_err = evaluate(model, sigma, val_dataloader, device)\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch {i} | Train NLL {epoch_nll} | Val NLL {val_nll} | Train Err {epoch_err} | Val Err {val_err}\")\n",
    "        if val_nll < best_val_nll:\n",
    "            best_val_nll = val_nll\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_epoch = i\n",
    "    \n",
    "    print(f\"Best epoch {best_epoch} | Val NLL {best_val_nll}\")       \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a MAP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 0.6844667196273804\n"
     ]
    }
   ],
   "source": [
    "data = UCIData(config.data.path)\n",
    "meta_data = data.get_metadata()\n",
    "device = get_device()\n",
    "train_dataloader, val_dataloader, test_dataloader = data.get_dataloaders(\n",
    "        dataset=config.data.name,\n",
    "        batch_size=config.trainer.batch_size,\n",
    "        seed=config.data.seed,\n",
    "        val_size=config.data.val_size,\n",
    "        split_index=config.data.split_index,\n",
    "        gap=(config.data.split == \"GAP\"),\n",
    "    )\n",
    "trainer = ModelTrainer(config.trainer, device=device)\n",
    "  \n",
    "\n",
    "model = create_mlp(\n",
    "        input_size=meta_data[config.data.name][\"input_dim\"],\n",
    "        hidden_sizes=config.model.hidden_sizes,\n",
    "        output_size=meta_data[config.data.name][\"output_dim\"],\n",
    "    )\n",
    "model = model.to(device=device, dtype=torch.float64)\n",
    "map_model, sigma = trainer.train(\n",
    "        model=model,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "    )\n",
    "print(f\"Sigma: {sigma}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Full Laplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior precision: 12.91549665014884\n",
      "Test NLL of Full Laplace Model: 1.1229294272116894\n"
     ]
    }
   ],
   "source": [
    "prior_precisions = np.logspace(0.1, 1, num=5, base=10).tolist()[:-1]  + np.logspace(1, 2, num=10, base=10).tolist()\n",
    "model_copy = copy.deepcopy(map_model)\n",
    "la, prior_precision = trainer.train_la_posthoc(\n",
    "                model=model_copy,\n",
    "                dataloader=train_dataloader,\n",
    "                subset_of_weights=\"all\",\n",
    "                hessian_structure=\"full\",\n",
    "                sigma_noise=sigma,\n",
    "                prior_mean=config.trainer.la.prior_mean,\n",
    "                val_dataloader=val_dataloader,\n",
    "                prior_precisions=prior_precisions\n",
    "            )\n",
    "posterior_covariance = la.posterior_covariance\n",
    "print(f\"Prior precision: {prior_precision}\")\n",
    "test_nll = trainer.evaluate_la(la, test_dataloader)\n",
    "print(f\"Test NLL of Full Laplace Model: {test_nll}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99 | Train NLL 1.4743169806260639 | Val NLL 1.244679601421234 | Train Err 0.8205121488803585 | Val Err 0.8112251779832637\n",
      "Epoch 199 | Train NLL 1.4773469885959392 | Val NLL 1.2425784576949006 | Train Err 0.8255731062484511 | Val Err 0.8050657749520898\n",
      "Epoch 299 | Train NLL 1.470458523124189 | Val NLL 1.2437502532741185 | Train Err 0.8200930783918375 | Val Err 0.8036910961462111\n",
      "Epoch 399 | Train NLL 1.4733108158127943 | Val NLL 1.2444934330380664 | Train Err 0.8228016941348018 | Val Err 0.8084724552324211\n",
      "Epoch 499 | Train NLL 1.4762731391387598 | Val NLL 1.2419099218647964 | Train Err 0.8270678294756338 | Val Err 0.7857737089917228\n",
      "Epoch 599 | Train NLL 1.475428097998921 | Val NLL 1.2451238848282793 | Train Err 0.8244963341500053 | Val Err 0.8009532876734328\n",
      "Epoch 699 | Train NLL 1.4767225985579688 | Val NLL 1.2473597660305986 | Train Err 0.8299873416660049 | Val Err 0.8048524221706379\n",
      "Epoch 799 | Train NLL 1.4722817645542527 | Val NLL 1.2429215914473866 | Train Err 0.8218057859844372 | Val Err 0.8067700858166391\n",
      "Epoch 899 | Train NLL 1.4738668041737524 | Val NLL 1.2409418184325491 | Train Err 0.8259548071431031 | Val Err 0.8066081651052206\n",
      "Epoch 999 | Train NLL 1.475089126930548 | Val NLL 1.2451091229161406 | Train Err 0.8205629727965223 | Val Err 0.8010631644904164\n",
      "Best epoch 0 | Val NLL 1.1168288341584975\n"
     ]
    }
   ],
   "source": [
    "refined_model = RefinedLaplace(model=map_model,\n",
    "                               output_dim=meta_data[config.data.name][\"output_dim\"],\n",
    "                               posterior_covariance=posterior_covariance)\n",
    "refined_model = train(model=refined_model,\n",
    "                      sigma=sigma,\n",
    "                      delta=prior_precision,\n",
    "                      train_dataloader=train_dataloader,\n",
    "                      val_dataloader=val_dataloader,\n",
    "                      epochs=1000,\n",
    "                      lr=1E-3,\n",
    "                      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test NLL of Refined Laplace Model: 1.1130633958749314\n"
     ]
    }
   ],
   "source": [
    "test_nll = evaluate_predictive(model=refined_model, sigma=sigma, dataloader=test_dataloader, device=device)\n",
    "print(f\"Test NLL of Refined Laplace Model: {test_nll}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "subnetwork_inference",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
